{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import mlx.nn as nn\n",
    "import mlx.core as mx\n",
    "from urllib import request\n",
    "import mlx.optimizers as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist(save_dir=\"./data\"):\n",
    "    def download_and_save(save_file):\n",
    "        base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "        filename = [\n",
    "                    [\"training_images\", \"train-images-idx3-ubyte.gz\"],\n",
    "                    [\"test_images\", \"t10k-images-idx3-ubyte.gz\"],\n",
    "                    [\"training_labels\", \"train-labels-idx1-ubyte.gz\"],\n",
    "                    [\"test_labels\", \"t10k-labels-idx1-ubyte.gz\"],\n",
    "                    ]\n",
    "\n",
    "        mnist = {}\n",
    "        for name in filename:\n",
    "            out_file = os.path.join(save_dir, name[1])\n",
    "            request.urlretrieve(base_url + name[1], out_file)\n",
    "        for name in filename[:2]:\n",
    "            out_file = os.path.join(save_dir, name[1])\n",
    "            with gzip.open(out_file, \"rb\") as f:\n",
    "                mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(\n",
    "                    -1, 28 * 28\n",
    "                )\n",
    "        for name in filename[-2:]:\n",
    "            out_file = os.path.join(save_dir, name[1])\n",
    "            with gzip.open(out_file, \"rb\") as f:\n",
    "                mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        with open(save_file, \"wb\") as f:\n",
    "            pickle.dump(mnist, f)\n",
    "\n",
    "    save_file = os.path.join(save_dir, \"mnist.pkl\")\n",
    "    if not os.path.exists(save_file):\n",
    "        download_and_save(save_file)\n",
    "    with open(save_file, \"rb\") as f:\n",
    "        mnist = pickle.load(f)\n",
    "\n",
    "    preproc = lambda x: x.astype(np.float32) / 255.0\n",
    "    mnist[\"training_images\"] = preproc(mnist[\"training_images\"])\n",
    "    mnist[\"test_images\"] = preproc(mnist[\"test_images\"])\n",
    "    return (\n",
    "        mnist[\"training_images\"],\n",
    "        mnist[\"training_labels\"].astype(np.uint32),\n",
    "        mnist[\"test_images\"],\n",
    "        mnist[\"test_labels\"].astype(np.uint32),\n",
    "    )\n",
    "\n",
    "train_x, train_y, test_x, test_y = mnist()\n",
    "assert train_x.shape == (60000, 28 * 28), \"Wrong training set size\"\n",
    "assert train_y.shape == (60000,), \"Wrong training set size\"\n",
    "assert test_x.shape == (10000, 28 * 28), \"Wrong test set size\"\n",
    "assert test_y.shape == (10000,), \"Wrong test set size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now build a simple MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "                self, \n",
    "                num_layers: int, \n",
    "                input_dim: int, \n",
    "                hidden_dim: int, \n",
    "                output_dim: int\n",
    "                ):\n",
    "        super().__init__()\n",
    "        layer_sizes = [input_dim] + [hidden_dim] * num_layers + [output_dim]\n",
    "        self.layers = [\n",
    "            nn.Linear(idim, odim)\n",
    "            for idim, odim in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers[:-1]:\n",
    "            x = mx.maximum(l(x), 0.0)\n",
    "        return self.layers[-1](x)\n",
    "\n",
    "\n",
    "def loss_fn(model, X, y):\n",
    "    return mx.mean(nn.losses.cross_entropy(model(X), y))\n",
    "\n",
    "\n",
    "def eval_fn(model, X, y):\n",
    "    return mx.mean(mx.argmax(model(X), axis=1) == y)\n",
    "\n",
    "\n",
    "def batch_iterate(batch_size, X, y):\n",
    "    perm = mx.array(np.random.permutation(y.size))\n",
    "    for s in range(0, y.size, batch_size):\n",
    "        ids = perm[s : s + batch_size]\n",
    "        yield X[ids], y[ids]\n",
    "\n",
    "\n",
    "def main():\n",
    "    seed = 0\n",
    "    num_layers = 2\n",
    "    hidden_dim = 32\n",
    "    num_classes = 10\n",
    "    batch_size = 256\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-1\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Load the data\n",
    "    train_images, train_labels, test_images, test_labels = map(mx.array, mnist())\n",
    "\n",
    "    # Load the model\n",
    "    model = MLP(num_layers, train_images.shape[-1], hidden_dim, num_classes)\n",
    "    mx.eval(model.parameters())\n",
    "\n",
    "    loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "    optimizer = optim.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        tic = time.perf_counter()\n",
    "        for X, y in batch_iterate(batch_size, train_images, train_labels):\n",
    "            loss, grads = loss_and_grad_fn(model, X, y)\n",
    "            optimizer.update(model, grads)\n",
    "            mx.eval(model.parameters(), optimizer.state)\n",
    "        accuracy = eval_fn(model, test_images, test_labels)\n",
    "        toc = time.perf_counter()\n",
    "        print(\n",
    "            f\"Epoch {e}: Test accuracy {accuracy.item():.3f},\"\n",
    "            f\" Time {toc - tic:.3f} (s)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Test accuracy 0.866, Time 0.138 (s)\n",
      "Epoch 1: Test accuracy 0.896, Time 0.124 (s)\n",
      "Epoch 2: Test accuracy 0.917, Time 0.126 (s)\n",
      "Epoch 3: Test accuracy 0.928, Time 0.128 (s)\n",
      "Epoch 4: Test accuracy 0.935, Time 0.131 (s)\n",
      "Epoch 5: Test accuracy 0.938, Time 0.128 (s)\n",
      "Epoch 6: Test accuracy 0.946, Time 0.132 (s)\n",
      "Epoch 7: Test accuracy 0.941, Time 0.133 (s)\n",
      "Epoch 8: Test accuracy 0.950, Time 0.133 (s)\n",
      "Epoch 9: Test accuracy 0.948, Time 0.128 (s)\n"
     ]
    }
   ],
   "source": [
    "mx.set_default_device(mx.cpu)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
